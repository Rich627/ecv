{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, functions\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downlaod Excel Plugine\n",
    "https://repo1.maven.org/maven2/com/crealytics/spark-excel_2.13/3.5.0_0.20.3/spark-excel_2.13-3.5.0_0.20.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "# raw_data_to_interns_orders = pd.read_excel(\"/Users/rich/Desktop/ecv proejct/Data/Raw Data_To interns_Orders.xlsx\")\n",
    "# raw_data_to_interns_contacts= pd.read_excel(\"/Users/rich/Desktop/ecv proejct/Data/Raw Data_To interns_Contacts.xlsx\")\n",
    "# raw_data_to_interns_products_orgin = pd.read_excel(\"/Users/rich/Desktop/ecv proejct/Data/Raw Data_To interns_Products_origin.xlsx\")\n",
    "\n",
    "# # covert to scv\n",
    "# raw_data_to_interns_orders.to_csv(\"Data/Raw Data_To interns_Orders.csv\", index=False, sep=\",\")\n",
    "# raw_data_to_interns_products_orgin.to_csv(\"Data/Raw Data_To interns_Products_origin.csv\", index=False, sep=\",\")\n",
    "# raw_data_to_interns_contacts.to_csv(\"Data/Raw Data_To interns_Contacts.csv\", index=False, sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = SparkConf().set(\"spark.jars\", \"/Users/dragonh/Workspace/intern/2024Q2/jars/spark-excel_2.13-3.5.0_0.20.3.jar\")\n",
    "# sc = SparkContext.getOrCreate(conf=conf)\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark==3.5.0\n",
    "# !pip install scala==2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/rich/miniconda3/envs/ecv:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "appnope                   0.1.4              pyhd8ed1ab_0    conda-forge\n",
      "arrow-cpp                 11.0.0          hce30654_56_cpu    conda-forge\n",
      "asttokens                 2.4.1              pyhd8ed1ab_0    conda-forge\n",
      "attrs                     23.1.0          py310hca03da5_0  \n",
      "aws-c-auth                0.7.8                h76f1ccf_2    conda-forge\n",
      "aws-c-cal                 0.6.9                hb1772db_2    conda-forge\n",
      "aws-c-common              0.9.10               h93a5062_0    conda-forge\n",
      "aws-c-compression         0.2.17               hb1772db_7    conda-forge\n",
      "aws-c-event-stream        0.3.2                hb5e90b3_8    conda-forge\n",
      "aws-c-http                0.7.14               hd747585_3    conda-forge\n",
      "aws-c-io                  0.13.36              h1112932_3    conda-forge\n",
      "aws-c-mqtt                0.9.10               h99ceed4_2    conda-forge\n",
      "aws-c-s3                  0.4.4                hc25d294_0    conda-forge\n",
      "aws-c-sdkutils            0.1.13               hb1772db_0    conda-forge\n",
      "aws-checksums             0.1.17               hb1772db_6    conda-forge\n",
      "aws-crt-cpp               0.25.0               h6d3774c_0    conda-forge\n",
      "aws-sdk-cpp               1.11.210             h87406ae_5    conda-forge\n",
      "awscli                    2.15.31         py310hbe9552e_0    conda-forge\n",
      "awscrt                    0.19.19         py310hea2a873_2    conda-forge\n",
      "awswrangler               3.7.1              pyhd8ed1ab_0    conda-forge\n",
      "backcall                  0.2.0              pyh9f0ad1d_0    conda-forge\n",
      "blas                      1.0                    openblas  \n",
      "boost-cpp                 1.82.0               h48ca7d4_2  \n",
      "boto3                     1.29.1          py310hca03da5_0  \n",
      "botocore                  1.32.1          py310hca03da5_0  \n",
      "bottleneck                1.3.7           py310hbda83bc_0  \n",
      "brotli                    1.0.7                hc377ac9_0  \n",
      "brotli-python             1.0.9           py310hc377ac9_7  \n",
      "bzip2                     1.0.8                h93a5062_5    conda-forge\n",
      "c-ares                    1.27.0               h93a5062_0    conda-forge\n",
      "ca-certificates           2024.3.11            hca03da5_0  \n",
      "certifi                   2024.2.2           pyhd8ed1ab_0    conda-forge\n",
      "cffi                      1.16.0          py310h80987f9_0  \n",
      "colorama                  0.4.6           py310hca03da5_0  \n",
      "comm                      0.2.2              pyhd8ed1ab_0    conda-forge\n",
      "contourpy                 1.2.0           py310h48ca7d4_0  \n",
      "cryptography              39.0.1          py310h834c97f_2  \n",
      "cycler                    0.11.0             pyhd3eb1b0_0  \n",
      "debugpy                   1.8.1           py310h692a8b6_0    conda-forge\n",
      "decorator                 5.1.1              pyhd8ed1ab_0    conda-forge\n",
      "distro                    1.8.0           py310hca03da5_0  \n",
      "docutils                  0.18.1          py310hca03da5_3  \n",
      "et_xmlfile                1.1.0           py310hca03da5_0  \n",
      "exceptiongroup            1.2.0              pyhd8ed1ab_2    conda-forge\n",
      "executing                 2.0.1              pyhd8ed1ab_0    conda-forge\n",
      "fonttools                 4.25.0             pyhd3eb1b0_0  \n",
      "freetype                  2.12.1               h1192e45_0  \n",
      "gflags                    2.2.2                h313beb8_1  \n",
      "glog                      0.6.0                h6da1cb0_0    conda-forge\n",
      "icu                       73.1                 h313beb8_0  \n",
      "idna                      3.4             py310hca03da5_0  \n",
      "importlib-metadata        7.1.0              pyha770c72_0    conda-forge\n",
      "importlib_metadata        7.1.0                hd8ed1ab_0    conda-forge\n",
      "ipykernel                 6.29.3             pyh3cd1d5f_0    conda-forge\n",
      "ipython                   8.17.2             pyh31c8845_0    conda-forge\n",
      "ipywidgets                8.1.2           py310hca03da5_0  \n",
      "jedi                      0.19.1             pyhd8ed1ab_0    conda-forge\n",
      "jmespath                  1.0.1           py310hca03da5_0  \n",
      "jpeg                      9e                   h80987f9_1  \n",
      "jsonschema                4.19.2          py310hca03da5_0  \n",
      "jsonschema-specifications 2023.7.1        py310hca03da5_0  \n",
      "jupyter_client            8.6.1              pyhd8ed1ab_0    conda-forge\n",
      "jupyter_core              5.7.2           py310hbe9552e_0    conda-forge\n",
      "jupyterlab_widgets        3.0.10          py310hca03da5_0  \n",
      "kiwisolver                1.4.4           py310h313beb8_0  \n",
      "krb5                      1.20.1               hf3e1bf2_1  \n",
      "lcms2                     2.12                 hba8e193_0  \n",
      "lerc                      3.0                  hc377ac9_0  \n",
      "libabseil                 20230802.1      cxx17_h13dd4ca_0    conda-forge\n",
      "libarrow                  11.0.0          hd4cc6db_56_cpu    conda-forge\n",
      "libboost                  1.82.0               h0bc93f9_2  \n",
      "libbrotlicommon           1.1.0                hb547adb_1    conda-forge\n",
      "libbrotlidec              1.1.0                hb547adb_1    conda-forge\n",
      "libbrotlienc              1.1.0                hb547adb_1    conda-forge\n",
      "libcrc32c                 1.1.2                hc377ac9_0  \n",
      "libcurl                   8.5.0                h3e2b118_0  \n",
      "libcxx                    16.0.6               h4653b0c_0    conda-forge\n",
      "libdeflate                1.17                 h80987f9_1  \n",
      "libedit                   3.1.20230828         h80987f9_0  \n",
      "libev                     4.33                 h1a28f6b_1  \n",
      "libevent                  2.1.12               h02f6b3c_1  \n",
      "libffi                    3.4.4                hca03da5_0  \n",
      "libgfortran               5.0.0           11_3_0_hca03da5_28  \n",
      "libgfortran5              11.3.0              h009349e_28  \n",
      "libgoogle-cloud           2.12.0               hfb399a7_4    conda-forge\n",
      "libgrpc                   1.59.3               h9560976_0    conda-forge\n",
      "libiconv                  1.16                 h1a28f6b_2  \n",
      "libnghttp2                1.57.0               h62f6fdd_0  \n",
      "libopenblas               0.3.21               h269037a_0  \n",
      "libpng                    1.6.39               h80987f9_0  \n",
      "libprotobuf               4.24.4               h810fc01_0    conda-forge\n",
      "libre2-11                 2023.09.01           h741fcf5_1    conda-forge\n",
      "libsodium                 1.0.18               h27ca646_1    conda-forge\n",
      "libsqlite                 3.45.2               h091b4b1_0    conda-forge\n",
      "libssh2                   1.10.0               h02f6b3c_2  \n",
      "libthrift                 0.19.0               h026a170_1    conda-forge\n",
      "libtiff                   4.5.1                h313beb8_0  \n",
      "libutf8proc               2.8.0                h1a8c8d9_0    conda-forge\n",
      "libwebp-base              1.3.2                h80987f9_0  \n",
      "libzlib                   1.2.13               h53f4e23_5    conda-forge\n",
      "llvm-openmp               14.0.6               hc6e5704_0  \n",
      "lz4-c                     1.9.4                h313beb8_0  \n",
      "matplotlib                3.8.0           py310hca03da5_0  \n",
      "matplotlib-base           3.8.0           py310h46d7db6_0  \n",
      "matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge\n",
      "munkres                   1.1.4                      py_0  \n",
      "nbformat                  5.9.2           py310hca03da5_0  \n",
      "ncurses                   6.4.20240210         h078ce10_0    conda-forge\n",
      "nest-asyncio              1.6.0              pyhd8ed1ab_0    conda-forge\n",
      "numexpr                   2.8.7           py310hecc3335_0  \n",
      "numpy                     1.26.4          py310h3b2db8e_0  \n",
      "numpy-base                1.26.4          py310ha9811e2_0  \n",
      "openjpeg                  2.3.0                h7a6adac_2  \n",
      "openpyxl                  3.0.10          py310h1a28f6b_0  \n",
      "openssl                   3.2.1                h0d3ecfb_1    conda-forge\n",
      "orc                       1.9.2                h7c018df_0    conda-forge\n",
      "packaging                 23.2               pyhd8ed1ab_0    conda-forge\n",
      "pandas                    2.1.4           py310h46d7db6_0  \n",
      "parso                     0.8.3              pyhd8ed1ab_0    conda-forge\n",
      "pexpect                   4.9.0              pyhd8ed1ab_0    conda-forge\n",
      "pickleshare               0.7.5                   py_1003    conda-forge\n",
      "pillow                    10.2.0          py310h80987f9_0  \n",
      "pip                       24.0               pyhd8ed1ab_0    conda-forge\n",
      "platformdirs              4.2.0              pyhd8ed1ab_0    conda-forge\n",
      "plotly                    5.19.0          py310h33ce5c2_0  \n",
      "prompt-toolkit            3.0.38             pyha770c72_0    conda-forge\n",
      "prompt_toolkit            3.0.38               hd8ed1ab_0    conda-forge\n",
      "psutil                    5.9.8           py310hd125d64_0    conda-forge\n",
      "ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\n",
      "pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge\n",
      "py4j                      0.10.9.7                 pypi_0    pypi\n",
      "pyarrow                   11.0.0          py310hbfed03b_1  \n",
      "pycparser                 2.21               pyhd3eb1b0_0  \n",
      "pygments                  2.17.2             pyhd8ed1ab_0    conda-forge\n",
      "pyopenssl                 23.0.0          py310hca03da5_0  \n",
      "pyparsing                 3.0.9           py310hca03da5_0  \n",
      "pysocks                   1.7.1           py310hca03da5_0  \n",
      "pyspark                   3.5.0                    pypi_0    pypi\n",
      "python                    3.10.14         h2469fbe_0_cpython    conda-forge\n",
      "python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\n",
      "python-fastjsonschema     2.16.2          py310hca03da5_0  \n",
      "python-tzdata             2023.3             pyhd3eb1b0_0  \n",
      "python_abi                3.10                    4_cp310    conda-forge\n",
      "pytz                      2023.3.post1    py310hca03da5_0  \n",
      "pyzmq                     25.1.2          py310hbb13138_0    conda-forge\n",
      "re2                       2023.09.01           h4cba328_1    conda-forge\n",
      "readline                  8.2                  h92ec313_1    conda-forge\n",
      "referencing               0.30.2          py310hca03da5_0  \n",
      "rpds-py                   0.10.6          py310hf0e4da2_0  \n",
      "ruamel.yaml               0.17.21         py310h1a28f6b_0  \n",
      "ruamel.yaml.clib          0.2.6           py310h1a28f6b_1  \n",
      "s3transfer                0.7.0           py310hca03da5_0  \n",
      "setuptools                69.2.0             pyhd8ed1ab_0    conda-forge\n",
      "six                       1.16.0             pyh6c4a22f_0    conda-forge\n",
      "snappy                    1.1.10               h313beb8_1  \n",
      "stack_data                0.6.2              pyhd8ed1ab_0    conda-forge\n",
      "tenacity                  8.2.2           py310hca03da5_0  \n",
      "tk                        8.6.13               h5083fa2_1    conda-forge\n",
      "tornado                   6.4             py310hd125d64_0    conda-forge\n",
      "traitlets                 5.14.2             pyhd8ed1ab_0    conda-forge\n",
      "typing-extensions         4.9.0           py310hca03da5_1  \n",
      "typing_extensions         4.9.0           py310hca03da5_1  \n",
      "tzdata                    2024a                h0c530f3_0    conda-forge\n",
      "urllib3                   1.26.18         py310hca03da5_0  \n",
      "wcwidth                   0.2.13             pyhd8ed1ab_0    conda-forge\n",
      "wheel                     0.43.0             pyhd8ed1ab_0    conda-forge\n",
      "widgetsnbextension        4.0.10          py310hca03da5_0  \n",
      "xz                        5.4.6                h80987f9_0  \n",
      "zeromq                    4.3.5                hebf3989_1    conda-forge\n",
      "zipp                      3.17.0             pyhd8ed1ab_0    conda-forge\n",
      "zlib                      1.2.13               h53f4e23_5    conda-forge\n",
      "zstd                      1.5.5                hd90d995_0  \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Runtime environment or build system does not support multi-release JARs. This will impact location-based features.\n",
      "Exception in thread \"main\" java.lang.NoSuchMethodError: 'scala.collection.convert.Decorators$AsScala scala.collection.JavaConverters$.mapAsScalaMapConverter(java.util.Map)'\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:171)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:114)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:108)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:76)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:84)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      4\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead Excel Data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/rich/miniconda3/lib/python3.10/site-packages/pyspark/jars/spark-excel_2.13-3.5.0_0.20.3.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# read data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m orders \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.crealytics.spark.excel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/rich/Desktop/ecv project/Data/Raw Data_To interns_Orders.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ecv/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/miniconda3/envs/ecv/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/miniconda3/envs/ecv/lib/python3.10/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ecv/lib/python3.10/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/miniconda3/envs/ecv/lib/python3.10/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(\"Read Excel Data\").setMaster(\"local[*]\").set(\"spark.jars\", \"/Users/rich/miniconda3/lib/python3.10/site-packages/pyspark/jars/spark-excel_2.13-3.5.0_0.20.3.jar\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate() \n",
    "# read data\n",
    "orders = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Users/rich/Desktop/ecv project/Data/Raw Data_To interns_Orders.xlsx\")\n",
    "\n",
    "orders.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option(\"maxRowsInMemory\", 20) \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/Users/rich/Desktop/ecv project/Data/Raw Data_To interns_Orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df = df.withColumnRenamed(\"item_system__c\", \"Product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scema code\n",
    "\n",
    "# from pyspark.sql.types import StructType, StringType, IntegerType, StructField\n",
    "# data = [(\"James\",\"\",\"Smith\",\"36\",\"M\",3000),\n",
    "#     (\"Michael\",\"Rose\",\"\",\"40\",\"M\",4000),\n",
    "#     (\"Robert\",\"\",\"Williams\",\"42\",\"M\",4000),\n",
    "#     (\"Maria\",\"Anne\",\"Jones\",\"39\",\"F\",4000),\n",
    "#     (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "#   ]\n",
    "\n",
    "# schema = StructType([ \n",
    "#     StructField(\"firstname\",StringType(),True), \n",
    "#     StructField(\"middlename\",StringType(),True), \n",
    "#     StructField(\"lastname\",StringType(),True), \n",
    "#     StructField(\"age\", StringType(), True), \n",
    "#     StructField(\"gender\", StringType(), True), \n",
    "#     StructField(\"salary\", IntegerType(), True) \n",
    "#   ])\n",
    " \n",
    "# df = spark.createDataFrame(data=data,schema=schema)\n",
    "# df.printSchema()\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change schema\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "df = df.withColumn(\"CreatedDate\", df[\"CreatedDate\"].cast(DateType()))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df.select('MailingCity').pandas_api().plot.pie(y='MailingCity')\n",
    "# df.groupBy('MailingCity').count().select('MailingCity', functions.col('count')).pandas_api().plot.pie(y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Product').count().select('Product', functions.col('count')).pandas_api().plot.pie(y=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT\n",
    "            *\n",
    "        FROM\n",
    "            test\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_create_analysis_df = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT\n",
    "            contact_formated.created_month as created_month,\n",
    "            COUNT(1) AS created_count\n",
    "        FROM (\n",
    "            SELECT\n",
    "                DATE_TRUNC('MM', to_timestamp(CreatedDate, 'M/d/yy HH:mm')) AS created_month\n",
    "            FROM\n",
    "                contact\n",
    "        ) contact_formated\n",
    "        GROUP BY\n",
    "            created_month\n",
    "        ORDER BY\n",
    "            created_month\n",
    "        ASC\n",
    "    \"\"\"\n",
    ")\n",
    "member_create_analysis_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_create_analysis_df.pandas_api().plot.line(x='created_month', y='created_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_create_analysis_df.write.format('parquet').option('path', 's3:xxxxx').saveAsTable('database.tablename')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
